llm: llama-3.2-1b-instruct
prep_command: cat
command: python3 $ROOT/scripts/nlp/main.py
command_env: $ROOT/scripts/nlp/env.yml
input: resources/free-response.tsv
output_dir: results/free-response
args: # The arguments to `command`
  - --max-tokens 100
  - --num-responses 2
  - --timeout 10
  - --model-category "llama"
  - --model-name "llama-3.2-1b-instruct"
shuffle: false
batch_size: 1
query_limit: 0 # Set to 0 to run all queries, otherwise only run this many (x the number of query templates)
query_fields:
  - species
query_templates:
  - Do you like {species}?
query_suffix: Explain it to me like I'm 5.
